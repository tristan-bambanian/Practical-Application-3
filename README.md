# Comparing Classifiers  
[View Jupyter Notebook](Comparing%20Classifiers.ipynb)

### Problem Statement  
Companies always want to get the most bang for their buck when they spend on marketing campaigns. In this analysis, I explore data from a Portuguese banking institution showing the results of multiple marketing campaigns. A successful result in this context would be if the contact agrees to subscribe a bank deposit. Since this result is a discrete variable (contact subscribes deposit or contact doesn't subscribe deposit) and we have labels in our dataset, this means it's a classification problem. I will compare multiple classification models (Logistic Regression, K-Nearest Neighbors, Decision Tree, Support Vector Machine) and evaluate their performance using various metrics to see which classifier (if any) is able to predict whether the contact will subscribe a deposit. I'll also be able to see which features were most important in predicting whether or not the contact will subscribe a deposit and this information could then be used to improve the efficiency of future marketing campaigns by allowing the bank to have a more focused, directed approach on what contacts to target.

### Data Preparation and EDA (Exploratory Data Analysis) 
The dataset didn't require too much cleaning to begin with. I just dropped some duplicate values and a lot of the columns had the value "unknown" rather than a missing value. I decided to leave this as is and just use one hot encoding to essentially treat "unknown" as another category for the categorical features. The feature "duration" was also dropped from the dataset as it was stated that this feature should be discarded for a realistic predictive model since it isn't known until after a call to the contact has been made.

Univariate analysis of the target variable indicated there was a class imbalance as only 11% of the data were successes (i.e. contact subscribed a deposit). Investing the numerical features with histograms and a correlation matrix showed that the variables with the strongest correlation to the target variable appeared to "nr.employed", "pdays", "euribor3m", and "emp.var.rate". An important finding here was that there did seem to be multicollinearity between "nr.employed", "euribor3m", and "emp.var.rate", so we should be weary of including all of them in our model. Looking at the categorical features with count plots and success rates among each feature's respective categories showed that the most useful columns appeared to be "job", "contact", "month", and "poutcome". The other categorical features seemed to hover around the same overall 11% success rate across its various categories.

For preprocessing, I first did a train test split with a test size of 20% as we had over 40,000 rows of data. Then I engineered the features by scaling the numerical features and one hot encoding the categorical features on the training data, then carrying the same transformation over to the test data to avoid data leakage.

### Modeling and Findings
I first established a baseline model using a dummy classifier that predicted the most frequent outcome. This baseline model had an accuracy of 89%, and precision, recall, and f1 scores were 0% since it would never predict a success for the target variable as it was the minority class.

I then built a simple Logistic Regression model with default parameters. The coefficients from the logistic regression model showed the log odds of the contact subscribing a deposit, and the strongest features were the emp.var.rate (employment variation rate), cons.price.idx (consumer price index), and when the month feature was March. The employment variation rate had a strong negative coefficient indicating that the contact was less likely to subscribe a deposit when the employment variation rate was high. On the other hand, when the month was March and the consumer price index was high, the contact was more likely to subscribe a deposit. In terms of performance, the simple Logistic Regression model achieved an accuracy score of 90%, precision score of 68%, recall score of 22%, and fF score of 33% against the test data.

Accuracy is not as meaningful for our problem since the data is very imbalanced (11% success rate), so precision, recall, and F1 scores are what we really want to look at. In this problem's context, I think a false negative is more detrimental than a false positive because if we predict the contact will subscribe a deposit but they don't, it's not as bad as if we predict the contact won't subscribe a deposit but they actually do. Predicting that the contact won't subscribe a deposit will probably result in the bank not including that contact in their next marketing campaign. The cost to the bank for missing out on these contacts that actually will subscribe a deposit is much higher than if they spent a bit more money on their campaign with contacts who they thought would subscribe, but actually won't. A mathematical way to evaluate this would be to model out the cost per contact for the campaign vs the estimated customer lifetime value (CLV) for each deposit they receive. For this reason, I think recall will be the most important performance metric for us to evalute since we want to try to correctly identify as many positives as possible and minimize false negatives. The only issue is our dataset is very imbalanced which isn't great for recall scores, so we will also want to pay attention to the F1 score for a blend of precision and recall.

Next, I added KNN (K-Nearest Neighbors), Decision Tree, and SVM (Support Vector Machine) models with defualt parameters to compare. My findings were:

- Training Time:
  - KNN, Logistic Regression, and Decision Tree were all very fast
  - SVM took a long time (over 5 minutes), but note that by default it uses the RBF kernel

- Performance:
  - Decision Tree had very high scores on the training data, but it's overfitting since the scores on test data were much lower
  - The next best model was KNN, but it also seems to be overfitting slightly as its precision, recall, and f1 test scores dropped quite a bit from its training scores
  - Logistic Regression and SVM both seemed to perform similarly and generalized well on the test data. However the accuracy score wasn't that much better than our baseline model, and the precision, recall, and f1 scores weren't very impressive, so we'll definitely want to do some hyperparameter tuning and potentially some additional feature engineering.

Finally, I implemented some hyperparameter tuning and cross validation using GridSearchCV along with some feature selection before testing the classifiers again. Tuning the max depth and minimum number of samples to split a node parameters on the decision tree and the number of neighbors, weights, and metric parameters on the KNN model helped stop the overfitting that was occurring. I also reduced the features down to 2 numerical and 4 categorical based on the earlier findings during EDA. But unfortunately, this didn't seem to really improve any of the models that much more than our simple logistic regression model with default parameters. All 4 classification models performed fairly similarly with accuracy scores around 90%, precision scores ranging from 57% - 70%, recall scores ranging from 22% to 29%, and F1 scores ranging from 34% to 38% against the test data. So I wouldn't necessarily say there was a best model. KNN performed notably worse in terms of precision compared to the other classifiers, but it did have the highest recall and F1 scores by a slight amount. From a practical standpoint, I would probably not recommend the SVM model since the training time was much longer and it didn't make up for it with significantly better performance than the other models.   

### Next Steps and Recommendations  
In terms of next steps, I think the best thing to do would be to go back and do some resampling to balance out the classes as this dataset had a significant class imbalance. I would also recommend trying out some better feature selection techniques like PCA (Principal Component Analysis) or RFE (Recursive Feature Elimination). This may be enough to help improve the model's performance. It may also be worth trying out some other, more powerful models like random forest or gradient boosting to see if they perform any better.   

Upon looking at feature importance, my recommendations to the Portuguese bank for their next marketing campaign would be to do it in the month of March and prioritize cellular phone types over telephone phone types. They should also keep an eye on economic metrics like the CPI (consumer price index) and employment variation rate as we saw with the logistic regression coefficients that the log odds of the outcome (contact subscribing a deposit) increased with a high CPI and decreased with a high employment variation rate.
